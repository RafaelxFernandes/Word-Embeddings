{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9497791",
   "metadata": {},
   "source": [
    "# Skip-gram with Negative Samples \n",
    "### by RafaelxFernandes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0256e815",
   "metadata": {},
   "source": [
    "## Downloading Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db548f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download() # go to the Corpora tab and double click on 'brown' and 'conll2000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8c540e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6789ded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Fulton',\n",
       "  'County',\n",
       "  'Grand',\n",
       "  'Jury',\n",
       "  'said',\n",
       "  'Friday',\n",
       "  'an',\n",
       "  'investigation',\n",
       "  'of',\n",
       "  \"Atlanta's\",\n",
       "  'recent',\n",
       "  'primary',\n",
       "  'election',\n",
       "  'produced',\n",
       "  '``',\n",
       "  'no',\n",
       "  'evidence',\n",
       "  \"''\",\n",
       "  'that',\n",
       "  'any',\n",
       "  'irregularities',\n",
       "  'took',\n",
       "  'place',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'jury',\n",
       "  'further',\n",
       "  'said',\n",
       "  'in',\n",
       "  'term-end',\n",
       "  'presentments',\n",
       "  'that',\n",
       "  'the',\n",
       "  'City',\n",
       "  'Executive',\n",
       "  'Committee',\n",
       "  ',',\n",
       "  'which',\n",
       "  'had',\n",
       "  'over-all',\n",
       "  'charge',\n",
       "  'of',\n",
       "  'the',\n",
       "  'election',\n",
       "  ',',\n",
       "  '``',\n",
       "  'deserves',\n",
       "  'the',\n",
       "  'praise',\n",
       "  'and',\n",
       "  'thanks',\n",
       "  'of',\n",
       "  'the',\n",
       "  'City',\n",
       "  'of',\n",
       "  'Atlanta',\n",
       "  \"''\",\n",
       "  'for',\n",
       "  'the',\n",
       "  'manner',\n",
       "  'in',\n",
       "  'which',\n",
       "  'the',\n",
       "  'election',\n",
       "  'was',\n",
       "  'conducted',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'September-October',\n",
       "  'term',\n",
       "  'jury',\n",
       "  'had',\n",
       "  'been',\n",
       "  'charged',\n",
       "  'by',\n",
       "  'Fulton',\n",
       "  'Superior',\n",
       "  'Court',\n",
       "  'Judge',\n",
       "  'Durwood',\n",
       "  'Pye',\n",
       "  'to',\n",
       "  'investigate',\n",
       "  'reports',\n",
       "  'of',\n",
       "  'possible',\n",
       "  '``',\n",
       "  'irregularities',\n",
       "  \"''\",\n",
       "  'in',\n",
       "  'the',\n",
       "  'hard-fought',\n",
       "  'primary',\n",
       "  'which',\n",
       "  'was',\n",
       "  'won',\n",
       "  'by',\n",
       "  'Mayor-nominate',\n",
       "  'Ivan',\n",
       "  'Allen',\n",
       "  'Jr.',\n",
       "  '.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = brown.sents()\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce252101",
   "metadata": {},
   "source": [
    "## Building embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855fb599",
   "metadata": {},
   "source": [
    "• sentences — The iterable over the tokenised sentences we will train on (the Brown sentences).\n",
    "\n",
    "• window — This determines which words are considered contexts of the target. For the window of size n the contexts are defined by capturing n words to the left of the target and n words to its right. The size of window will affect the type of similarity captured in the embeddings — bigger windows will result in more topical/domain similarities.\n",
    "\n",
    "• min_count — We can use this parameter to tell the model to ignore some infrequent words — don’t create an embedding for them and don’t include them as contexts. The min_count defines a threshold frequency value that needs to be reached for the word to be included in the vocabulary.\n",
    "\n",
    "• negative — Defines the number of negative samples (incorrect training pair instances) that are drawn for each good sample.\n",
    "\n",
    "• workers — Determines how many worker threads will be used to train the model.\n",
    "\n",
    "In our setting for window and negative samples we will follow the settings from the original Skip-gram papers. We will set the workers parameter to the number of available cores and train our model for ten epochs (as our training data is quite small, ~1M words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e711774",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(\n",
    "    sentences,\n",
    "    window = 5,\n",
    "    min_count = 5,\n",
    "    negative = 15,\n",
    "    workers = multiprocessing.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f9c2a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'Saturday':\n",
      "  [('Monday', 0.9549916386604309), ('Sunday', 0.9446992874145508), ('Friday', 0.9341464638710022), ('Tuesday', 0.9257744550704956), ('fourth', 0.9210171103477478), ('Wednesday', 0.9108353853225708), ('ending', 0.9106801152229309), ('December', 0.9002511501312256), ('afternoon', 0.8998737931251526), ('April', 0.898629903793335)]\n"
     ]
    }
   ],
   "source": [
    "# Get trained embeddings - a KeyedVector instance\n",
    "word_vectors = w2v.wv\n",
    "result = word_vectors.similar_by_word('Saturday')\n",
    "print(\"Most similar to 'Saturday':\\n \", result[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0844f2",
   "metadata": {},
   "source": [
    "## Using embedding as feature in a Neural model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf356a0a",
   "metadata": {},
   "source": [
    "Now that we have our embeddings it’s time to put them into use. We will use them as features for the part-of-speech (POS) tagging model we will develop. A part-of-speech is a grammatical category of a word, such as a noun, verb or an adjective. Given a sequence of words, the task is to label each of them with a suitable POS tag.\n",
    "\n",
    "We will build a simple neural model for multi-class classification. For now, we will ignore the context of the word we are tagging — our network will take only one word as input and output the probability distribution over all possible POS tags. To train and evaluate our model we will make use of yet another NLTK resource: the data from the CONLL-2000 Shared Task, which has been annotated with POS tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba6f71f",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d7344b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Activation, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import collections\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "139b862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = conll2000.tagged_words(\"train.txt\")\n",
    "test_words = conll2000.tagged_words(\"test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71c32a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Confidence', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('pound', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('widely', 'RB'),\n",
       " ('expected', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('take', 'VB'),\n",
       " ('another', 'DT')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6068b",
   "metadata": {},
   "source": [
    "Our first step is to process this data into a model-friendly format — replace all words and tags with their corresponding indexes and split the data into inputs and outputs (tag labels). To do that we will need a dictionary which maps words to their corresponding ids and a similar dictionary for the tags. We will create the latter based on our CONLL training data, but to create the first we will use the vocabulary of our trained embedding model — as it should only contain the words which we are able to represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e243d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts text in the form of (word, pos) tuples\n",
    "# and returns a dictionary mapping POS-tags to unique ids\n",
    "def get_tag_vocabulary(tagged_words):\n",
    "    \n",
    "    tag2id = {}\n",
    "    \n",
    "    for item in tagged_words:\n",
    "        tag = item[1]\n",
    "        tag2id.setdefault(tag, len(tag2id))\n",
    "    \n",
    "    return tag2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "405a753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The word_vectors.key_to_index dictionary stores integers\n",
    "word2id = {key: value for key, value in word_vectors.key_to_index.items()}\n",
    "tag2id = get_tag_vocabulary(train_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493604c",
   "metadata": {},
   "source": [
    "Adding a new word to our vocabulary — the ‘UNK’, which will represent all words we don’t have an embedding for. But adding this word to the vocabulary means it will need to have a corresponding embedding, not present in our representations. One solution would be to retrain Skip-gram after having replaced some occurrences of low frequency words in our training data with an ‘UNK’ token. But we will approach this problem from a different angle by approximating the UNK’s vector with a mean of all existing embeddings. After doing so, we will add this new representation to the matrix of all other embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75827ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_INDEX = 0\n",
    "UNK_TOKEN = \"UNK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "570bda14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds a new word to the existing matrix of word embeddings\n",
    "def add_new_word(new_word, new_vector, new_index, embedding_matrix, word2id):\n",
    "    \n",
    "    # Inserting the vector before given index, along axis 0\n",
    "    embedding_matrix = np.insert(embedding_matrix, [new_index], [new_vector], axis = 0)\n",
    "    \n",
    "    # Updating the indexes of words that follow the new word\n",
    "    word2id = {word: (index + 1) if index >= new_index else index\n",
    "              for word, index in word2id.items()}\n",
    "    word2id[new_word] = new_index\n",
    "    \n",
    "    return embedding_matrix, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ebf291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = word_vectors.vectors\n",
    "unk_vector = embedding_matrix.mean(0)\n",
    "embedding_matrix, word2id = add_new_word(UNK_TOKEN, unk_vector, UNK_INDEX, embedding_matrix, word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4705698",
   "metadata": {},
   "source": [
    "Now it’s time to get our integer, model-friendly data — both for the train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dffe1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces all words and tags with their corresponding ids and\n",
    "# separates words(features) from the tags(labels)\n",
    "def get_int_data(tagged_words, word2id, tag2id):\n",
    "    \n",
    "    # X holds word ids, Y hold their tags ids\n",
    "    X, Y = [], []\n",
    "    \n",
    "    # Variable to keep track of the number of unknown words\n",
    "    # which are words we don't have a representation for\n",
    "    unk_count = 0\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        Y.append(tag2id.get(tag))\n",
    "        \n",
    "        if word in word2id:\n",
    "            X.append(word2id.get(word))\n",
    "        else:\n",
    "            X.append(UNK_INDEX)\n",
    "            unk_count += 1\n",
    "            \n",
    "    X = np.asarray(X).astype(np.float32)\n",
    "    Y = np.asarray(Y).astype(np.float32)\n",
    "\n",
    "    print(\"Data created. Percentage of unknown words: %.3f\" % (unk_count/ len(tagged_words)))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7c24317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created. Percentage of unknown words: 0.143\n",
      "Data created. Percentage of unknown words: 0.149\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = get_int_data(train_words, word2id, tag2id)\n",
    "X_test, Y_test = get_int_data(test_words, word2id, tag2id)\n",
    "\n",
    "Y_train, Y_test = to_categorical(Y_train), to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c10903",
   "metadata": {},
   "source": [
    "### Defining and training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f8fc65",
   "metadata": {},
   "source": [
    "Our next step is to define the model for POS classification. We will do so using TensorFlow’s implementation of the Keras API. Our model will take as input an index into the word embedding matrix, which will be used to look up the appropriate embedding. It will have one hidden layer with the tanh activation function and at the final layer will use the softmax activation — outputting a probability distribution over all possible tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8db06bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 100\n",
    "HIDDEN_SIZE = 50\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85f0c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and returns a simple part-of-speech model,\n",
    "# which takes only one word as input\n",
    "def define_model(embedding_matrix, class_count):\n",
    "    \n",
    "    vocab_length = len(embedding_matrix)\n",
    "    \n",
    "    # Sequential model is a stack of layers, we will add them one by one\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding( # Layer which turns word indexes into vectors\n",
    "        input_dim = vocab_length,\n",
    "        output_dim = EMB_DIM, # output of this layer is the embedding of the input word\n",
    "        weights = [embedding_matrix], # matrix holding the trained embeddings\n",
    "        input_length = 1)) # specify how many indexes we are looking for\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(HIDDEN_SIZE))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.add(Dense(class_count))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer = tf.optimizers.Adam(),\n",
    "                 loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "                 metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9059e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1, 100)            1517400   \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                5050      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 44)                2244      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 44)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,524,694\n",
      "Trainable params: 1,524,694\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pos_model = define_model(embedding_matrix, len(tag2id))\n",
    "pos_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e29b6984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655/1655 [==============================] - 52s 31ms/step - loss: 0.7681 - accuracy: 0.7932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a597383bb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "pos_model.fit(X_train, Y_train, batch_size = BATCH_SIZE, epochs = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab68cc1",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e430b",
   "metadata": {},
   "source": [
    "Now that we have a trained model it’s time to see how well it’s performing on the unseen data. We will use it to tag the words from the test data and calculate the accuracy of its predictions: the ratio of the number of correct tags to the number of all words in the test set. To get more insight, we will also determine what are the most commonly mistagged words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd1d4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the given model by computing the accuracy of its predictions\n",
    "# on the given test data and prints out 10 most mistagged words\n",
    "def evaluate_model(model, id2word, x_test, y_test):\n",
    "    \n",
    "    _, acc = model.evaluate(x_test, y_test)\n",
    "    print(\"Accuracy: %.4f\" % acc)\n",
    "    \n",
    "    # Get model predictions and count its erros\n",
    "    y_pred = np.argmax(model.predict(x_test), axis = -1)\n",
    "    error_counter = collections.Counter()\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        correct_tag_id = np.argmax(y_test[i]) # turn a one-hot encoding to an index\n",
    "        \n",
    "        if y_pred[i] != correct_tag_id:\n",
    "            word = id2word[int(x_test[i])]\n",
    "            error_counter[word] += 1\n",
    "            \n",
    "    print(\"Most common errors: \\n\", error_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91b52fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1481/1481 [==============================] - 2s 2ms/step - loss: 0.5061 - accuracy: 0.8479\n",
      "Accuracy: 0.8479\n",
      "Most common errors: \n",
      " [('UNK', 5034), ('that', 136), ('have', 51), ('as', 37), ('more', 30), ('about', 18), ('executive', 18), ('American', 18), ('plans', 16), ('called', 14)]\n"
     ]
    }
   ],
   "source": [
    "id2word = sorted(word2id, key = word2id.get)\n",
    "evaluate_model(pos_model, id2word, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f89d02",
   "metadata": {},
   "source": [
    "As expected, our model performs the worst when tagging the unknown words. The accuracy is 85%, which is not too bad, but we can do better. Let’s try improving the model by making the classification context-dependent!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7c5b4",
   "metadata": {},
   "source": [
    "### Building a context-dependent model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3024e9",
   "metadata": {},
   "source": [
    "We will now alter the model built in the previous steps to take more than one word index as input. In addition to the index of the classified word we will feed in the indexes of two words to its left side and two words to its right side — all in the order of their appearance in the training data.\n",
    "\n",
    "Apart from redefining our model we also need to adjust the way we process the CONLL data: the X_train and X_test will now consist of arrays of indexes, rather than single indexes. We will use a sliding-window approach to retrieve all word spans of length 5 — each consisting of the tagged word and its context-words. For each such span, the corresponding label will be the tag of the middle word. To represent the missing contexts of words at the beginning and the end of the training data sequence we will use a new, special word — the end-of-sequence (EOS). We will add EOS using the previously defined add_new_word function, in a similar way to how we have added UNK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9108777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_INDEX = 1\n",
    "EOS_TOKEN = \"EOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a810981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a random EOS vector\n",
    "eos_vector = np.random.standard_normal(EMB_DIM)\n",
    "embedding_matrix, word2id = add_new_word(EOS_TOKEN, eos_vector, EOS_INDEX, embedding_matrix, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "813df7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined the size of the context window\n",
    "CONTEXT_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24ace3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replaces all words and tags with their corresponding ids and\n",
    "# generates an array of label ids Y and the training data X,\n",
    "# which consists of arrays of word indexes (of tagged word and its context)\n",
    "def get_window_int_data(tagged_words, word2id, tag2id):\n",
    "    \n",
    "    # X holds word ids, Y hold their tags ids\n",
    "    X, Y = [], []\n",
    "    \n",
    "    # Variable to keep track of the number of unknown words\n",
    "    # which are words we don't have a representation for\n",
    "    unk_count = 0\n",
    "    \n",
    "    # The complete span of the sliding window\n",
    "    span = 2 * CONTEXT_SIZE + 1\n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    padding = [(EOS_TOKEN, None)] * CONTEXT_SIZE\n",
    "    buffer += padding + tagged_words[:CONTEXT_SIZE]\n",
    "    \n",
    "    for item in (tagged_words[CONTEXT_SIZE:] + padding):\n",
    "        buffer.append(item)\n",
    "        \n",
    "        # The input to the model is the ids of all words in the window\n",
    "        window_ids = np.array([word2id.get(word) if (word in word2id) else UNK_INDEX\n",
    "                              for (word, _) in buffer])\n",
    "        \n",
    "        X.append(window_ids)\n",
    "        \n",
    "        # The label is the tag of the middle word\n",
    "        middle_word, middle_tag = buffer[CONTEXT_SIZE]\n",
    "        Y.append(tag2id.get(middle_tag))\n",
    "        \n",
    "        if middle_word not in word2id:\n",
    "            unk_count += 1\n",
    "            \n",
    "    X = np.asarray(X).astype(np.float32)\n",
    "    Y = np.asarray(Y).astype(np.float32)\n",
    "\n",
    "    print(\"Data created. Percentage of unknown words: %.3f\" % (unk_count/ len(tagged_words)))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6980d8f0",
   "metadata": {},
   "source": [
    "Our next step is defining the model. It will be very similar to the simple model from our previous steps. In fact, the only thing that will change is the Embedding layer, which will now take 5 word indexes instead of 1. We will also slightly alter our evaluation function — to support the structure of our new training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73fd9059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and returns a simple part-of-speech model,\n",
    "# which takes only one word as input\n",
    "def define_context_sensitive_model(embedding_matrix, class_count):\n",
    "    \n",
    "    vocab_length = len(embedding_matrix)\n",
    "    total_span = CONTEXT_SIZE * 2 + 1\n",
    "    \n",
    "    # Sequential model is a stack of layers, we will add them one by one\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding( # Layer which turns word indexes into vectors\n",
    "        input_dim = vocab_length,\n",
    "        output_dim = EMB_DIM, # output of this layer is the embedding of the input word\n",
    "        weights = [embedding_matrix], # matrix holding the trained embeddings\n",
    "        input_length = total_span)) # specify how many indexes we are looking for\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(HIDDEN_SIZE))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.add(Dense(class_count))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer = tf.optimizers.Adam(),\n",
    "                 loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "                 metrics = [\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0b54ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluates the given model by computing the accuracy of its predictions\n",
    "# on the given test data and prints out 10 most mistagged words\n",
    "def evaluate_context_sensitive_model(model, id2word, x_test, y_test):\n",
    "    \n",
    "    _, acc = model.evaluate(x_test, y_test)\n",
    "    print(\"Accuracy: %.4f\" % acc)\n",
    "    \n",
    "    # Get model predictions and count its erros\n",
    "    y_pred = np.argmax(model.predict(x_test), axis = -1)\n",
    "    error_counter = collections.Counter()\n",
    "    \n",
    "    for i in range(len(x_test)):\n",
    "        correct_tag_id = np.argmax(y_test[i]) # turn a one-hot encoding to an index\n",
    "        \n",
    "        if y_pred[i] != correct_tag_id:\n",
    "            if isinstance(x_test[i], np.ndarray):\n",
    "                word = id2word[int(x_test[i][CONTEXT_SIZE])]\n",
    "            else:\n",
    "                word = id2word[int(x_test[i])]\n",
    "            \n",
    "            error_counter[word] += 1\n",
    "            \n",
    "    print(\"Most common errors: \\n\", error_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41e6caa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created. Percentage of unknown words: 0.143\n",
      "Data created. Percentage of unknown words: 0.149\n"
     ]
    }
   ],
   "source": [
    "X_train2, Y_train2 = get_window_int_data(train_words, word2id, tag2id)\n",
    "X_test2, Y_test2 = get_window_int_data(test_words, word2id, tag2id)\n",
    "\n",
    "Y_train2, Y_test2 = to_categorical(Y_train2), to_categorical(Y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad9c4fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1655/1655 [==============================] - 54s 33ms/step - loss: 0.6182 - accuracy: 0.8320\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a59459b280>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_pos_model = define_context_sensitive_model(embedding_matrix, len(tag2id))\n",
    "cs_pos_model.fit(X_train2, Y_train2, batch_size = BATCH_SIZE, epochs = 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6571e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1481/1481 [==============================] - 3s 2ms/step - loss: 0.3162 - accuracy: 0.9055\n",
      "Accuracy: 0.9055\n",
      "Most common errors: \n",
      " [('UNK', 2978), ('is', 67), ('out', 30), ('so', 19), ('old', 14), ('content', 10), ('boot', 9), ('It', 8), ('C.', 8), ('its', 8)]\n"
     ]
    }
   ],
   "source": [
    "evaluate_context_sensitive_model(cs_pos_model, id2word, X_test2, Y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a02a59",
   "metadata": {},
   "source": [
    "Our accuracy jumped up to 91%! It looks like adding the context really helped with tagging the unknown words and also helped to disambiguate other words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d9cab8",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- Skip-Gram: NLP context words prediction algorithm: https://towardsdatascience.com/skip-gram-nlp-context-words-prediction-algorithm-5bbf34f84e0c\n",
    "\n",
    "- Word2Vec (skip-gram model): PART 1 - Intuition.: https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b\n",
    "\n",
    "- Word2Vec (Skip-Gram model) Explained: https://medium.datadriveninvestor.com/word2vec-skip-gram-model-explained-383fa6ddc4ae\n",
    "\n",
    "- Word2Vec Tutorial Part 2 - Negative Sampling: http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/\n",
    "\n",
    "- NLP’s word2vec: Negative Sampling Explained: https://www.baeldung.com/cs/nlps-word2vec-negative-sampling\n",
    "\n",
    "- NLP 102: Negative Sampling and GloVe: https://towardsdatascience.com/nlp-101-negative-sampling-and-glove-936c88f3bc68"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
